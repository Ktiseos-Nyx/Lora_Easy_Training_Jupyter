{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ **Unified LoRA Trainer** - One Notebook to Rule Them All!\n",
    "\n",
    "**‚ú® Auto-detects your model type and uses the optimal Kohya training script automatically!**\n",
    "\n",
    "## üß† **Smart Detection System**\n",
    "- **SD 1.5/2.0**: Uses `train_network.py` \n",
    "- **SDXL**: Uses `sdxl_train_network.py`\n",
    "- **Flux**: Uses `flux_train_network.py` with bf16 precision\n",
    "- **SD3**: Uses `sd3_train_network.py` with T5 text encoder support\n",
    "\n",
    "## üé® **What This Notebook Does**\n",
    "1. **Cell 1**: Environment Setup & Backend Installation\n",
    "2. **Cell 2**: Universal Training Configuration (auto-adapts to your model)\n",
    "3. **Cell 3**: Training Execution & Monitoring\n",
    "4. **Cell 4**: Post-Training Utilities (resize, upload, etc.)\n",
    "\n",
    "## üî¨ **For Advanced Users**\n",
    "Need specialized features? Check out:\n",
    "- `Flux_SD3_Trainer.ipynb` - T5 text encoder fine-tuning\n",
    "- `T5_Trainer.ipynb` - Dedicated T5 training\n",
    "- Future: SimpleTuner notebooks for bleeding-edge models\n",
    "\n",
    "---\n",
    "**üí° Powered by Kohya's battle-tested library system + our user-friendly widgets!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **CELL 1:** üèóÔ∏è Environment Setup & Backend Installation\n",
    "\n",
    "**Run this first!** Sets up the training environment and downloads the Kohya backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **CELL 1:** Environment Setup & Backend Installation\n",
    "# This sets up the Kohya backend and all required dependencies\n",
    "# Auto-detects your system configuration for optimal performance\n",
    "\n",
    "from shared_managers import get_setup_manager, get_model_manager\n",
    "from widgets.setup_widget import SetupWidget\n",
    "\n",
    "print(\"üéØ UNIFIED LORA TRAINER - Initializing...\")\n",
    "print(\"‚ú® Auto-detection system: ON\")\n",
    "print(\"üß† Kohya backend integration: READY\")\n",
    "print()\n",
    "\n",
    "# Create setup widget with shared managers\n",
    "setup_widget = SetupWidget(get_setup_manager(), get_model_manager())\n",
    "setup_widget.display()\n",
    "\n",
    "print()\n",
    "print(\"üí° This notebook automatically detects your model type and uses:\")\n",
    "print(\"   ‚Ä¢ SD 1.5/2.0 ‚Üí train_network.py\")\n",
    "print(\"   ‚Ä¢ SDXL ‚Üí sdxl_train_network.py\") \n",
    "print(\"   ‚Ä¢ Flux ‚Üí flux_train_network.py\")\n",
    "print(\"   ‚Ä¢ SD3 ‚Üí sd3_train_network.py\")\n",
    "print(\"   No manual configuration needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **CELL 2:** üé® Universal Training Configuration\n",
    "\n",
    "**The magic happens here!** Configure your training settings and the system will automatically:\n",
    "- Detect your model type from the model path\n",
    "- Select the optimal training script\n",
    "- Apply architecture-specific optimizations\n",
    "- Use the right precision settings (fp16 for SD/SDXL, bf16 for Flux, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **CELL 2:** Universal Training Configuration Widget\n",
    "# This auto-adapts based on your model type - no manual configuration needed!\n",
    "# Uses the new Kohya-powered backend with automatic model detection\n",
    "\n",
    "from core.refactored_training_manager import HybridTrainingManager\n",
    "from widgets.training_widget import TrainingWidget\n",
    "\n",
    "print(\"üéØ UNIVERSAL TRAINING CONFIGURATION\")\n",
    "print(\"‚ú® Smart model detection enabled\")\n",
    "print(\"üîß Kohya optimization system active\")\n",
    "print()\n",
    "\n",
    "# Create the unified training manager (uses Kohya backend with auto-detection)\n",
    "training_manager = HybridTrainingManager()\n",
    "training_widget = TrainingWidget(training_manager)\n",
    "\n",
    "# Display the universal configuration widget\n",
    "training_widget.display()\n",
    "\n",
    "print()\n",
    "print(\"üí° Tips:\")\n",
    "print(\"   ‚Ä¢ Just select your model file - type detection is automatic!\")\n",
    "print(\"   ‚Ä¢ Default settings are optimized for each architecture\")\n",
    "print(\"   ‚Ä¢ All Kohya optimizers and features available\")\n",
    "print(\"   ‚Ä¢ CAME optimizer auto-detects if available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **CELL 3:** üöÄ Training Execution & Real-time Monitoring\n",
    "\n",
    "**Ready to train?** This cell:\n",
    "- Uses the configuration from Cell 2\n",
    "- Automatically selects the right Kohya training script\n",
    "- Shows real-time progress with loss curves and metrics\n",
    "- Handles all the technical details behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **CELL 3:** Training Execution & Monitoring\n",
    "# Run this AFTER configuring your settings in Cell 2\n",
    "# The system automatically uses the optimal training script for your model type\n",
    "\n",
    "from shared_managers import get_training_manager\n",
    "from widgets.training_monitor_widget import TrainingMonitorWidget\n",
    "\n",
    "print(\"üöÄ UNIFIED TRAINING SYSTEM - Starting...\")\n",
    "print(\"üß† Model type will be auto-detected from your model file\")\n",
    "print(\"‚ö° Optimal Kohya script will be selected automatically\")\n",
    "print(\"üìä Real-time monitoring enabled\")\n",
    "print()\n",
    "\n",
    "# Use the shared training manager instance (same as Cell 2)\n",
    "training_monitor = TrainingMonitorWidget(training_manager_instance=get_training_manager())\n",
    "\n",
    "# Display the monitoring widget\n",
    "training_monitor.display()\n",
    "\n",
    "print()\n",
    "print(\"üí° What happens automatically:\")\n",
    "print(\"   ‚úÖ Model type detection from your model path\")\n",
    "print(\"   ‚úÖ Optimal training script selection\")\n",
    "print(\"   ‚úÖ Architecture-specific optimizations\")\n",
    "print(\"   ‚úÖ Memory and precision settings\")\n",
    "print(\"   ‚úÖ Real-time progress tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **CELL 4:** üõ†Ô∏è Post-Training Utilities\n",
    "\n",
    "**Training complete?** Use these utilities to:\n",
    "- Resize your LoRA for different sizes\n",
    "- Upload to HuggingFace Hub\n",
    "- Test your trained LoRA\n",
    "- Organize your training outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **CELL 4:** Post-Training Utilities\n",
    "# Resize, upload, and manage your trained LoRAs\n",
    "# Works with all model types automatically\n",
    "\n",
    "from shared_managers import get_utilities_manager\n",
    "from widgets.utilities_widget import UtilitiesWidget\n",
    "\n",
    "print(\"üõ†Ô∏è POST-TRAINING UTILITIES\")\n",
    "print(\"üì¶ LoRA management and optimization tools\")\n",
    "print(\"‚òÅÔ∏è HuggingFace Hub integration\")\n",
    "print()\n",
    "\n",
    "# Create utilities widget\n",
    "utilities_widget = UtilitiesWidget(get_utilities_manager())\n",
    "utilities_widget.display()\n",
    "\n",
    "print()\n",
    "print(\"üéâ Training Complete! Your LoRA is ready to use!\")\n",
    "print(\"üí° For advanced features, check out the specialized notebooks:\")\n",
    "print(\"   ‚Ä¢ Flux_SD3_Trainer.ipynb - T5 text encoder training\")\n",
    "print(\"   ‚Ä¢ Future SimpleTuner integration for bleeding-edge models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **üìä Optional: LoRA Step Calculator**\n",
    "\n",
    "**Want to optimize your training parameters?** This calculator helps you find the perfect settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **OPTIONAL:** LoRA Step Calculator\n",
    "# Calculate optimal training steps, learning rates, and batch sizes\n",
    "# Works for all model architectures\n",
    "\n",
    "from shared_managers import create_widget\n",
    "\n",
    "print(\"üìä LORA STEP CALCULATOR\")\n",
    "print(\"üßÆ Optimize your training parameters\")\n",
    "print()\n",
    "\n",
    "calculator_widget = create_widget('calculator')\n",
    "calculator_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **üóÇÔ∏è Optional: File Management**\n",
    "\n",
    "**Need to manage your training files?** Upload datasets, organize outputs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **OPTIONAL:** File Management Utilities\n",
    "# Upload datasets, manage training files, organize outputs\n",
    "\n",
    "from shared_managers import create_widget\n",
    "\n",
    "print("üóÇÔ∏è FILE MANAGEMENT SYSTEM")\n",
    "print("üìÅ Upload and organize training data")\n",
    "print()\n",
    "\n",
    "file_manager_widget = create_widget('file_manager')\n",
    "display(file_manager_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Optional: LoRA Inference Preview\n",
    "\n",
    "**Test your trained LoRAs directly in the notebook!** Generate images with your LoRA and see the results instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **OPTIONAL:** LoRA Inference Preview\n",
    "# Generate images with your trained LoRAs\n",
    "\n",
    "from widgets.inference_widget import InferenceWidget\n",
    "\n",
    "print("üé® LORA INFERENCE PREVIEW")\n",
    "print("‚ú® Generate images with your trained LoRAs")\n",
    "print()\n",
    "\n",
    "inference_widget = InferenceWidget()\n",
    "inference_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ **Congratulations!**\n",
    "\n",
    "You've successfully trained a LoRA using the **Unified Training System**!\n",
    "\n",
    "## **‚ú® What Just Happened**\n",
    "- Your model type was automatically detected\n",
    "- The optimal Kohya training script was selected\n",
    "- Architecture-specific optimizations were applied\n",
    "- Training used battle-tested Kohya backend code\n",
    "\n",
    "## **üéØ For Advanced Users**\n",
    "Need more control or specialized features? Check out:\n",
    "- **`Flux_SD3_Trainer.ipynb`** - T5 text encoder training, advanced Flux/SD3 features\n",
    "- **`T5_Trainer.ipynb`** - Dedicated T5 text encoder fine-tuning\n",
    "- **Future SimpleTuner notebooks** - For bleeding-edge models not supported by Kohya\n",
    "\n",
    "## **ü§ù Contributing**\n",
    "Found a bug or want to add features? This project benefits from the entire community!\n",
    "\n",
    "---\n",
    "*Powered by Kohya-ss, LyCORIS, and Derrian Distro backend systems*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}