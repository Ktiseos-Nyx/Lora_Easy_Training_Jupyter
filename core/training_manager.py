# core/training_manager.py
import subprocess
import os
import sys
import toml
import json
from typing import Dict, List, Optional, Any

class HybridTrainingManager:
    """üß™ FRANKENSTEIN TRAINING MANAGER üí•
    
    Combines the best features from:
    - Kohya-ss (stable foundation)
    - LyCORIS (advanced methods)
    - Derrian Distro (CAME + REX)
    - OneTrainer (fused back pass)
    - HakuLatent (future research)
    """
    
    def __init__(self):
        # Use current working directory to match where notebook is running
        self.project_root = os.getcwd()
        self.trainer_dir = os.path.join(self.project_root, "trainer")
        self.config_dir = os.path.join(self.project_root, "training_configs")  # Much better name!
        self.sd_scripts_dir = os.path.join(self.trainer_dir, "derrian_backend", "sd_scripts")
        self.output_dir = os.path.join(self.project_root, "output")
        self.logging_dir = os.path.join(self.project_root, "logs")
        os.makedirs(self.config_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.logging_dir, exist_ok=True)
        self.process = None
        
        # üîß Setup Python path for Derrian's custom optimizers
        self._setup_custom_optimizers()
        
        # üî¨ Advanced feature support
        self.advanced_optimizers = self._init_advanced_optimizers()
        self.standard_optimizers = self._init_standard_optimizers()
        self.lycoris_methods = self._init_lycoris_methods()
        self.experimental_features = self._init_experimental_features()
        self.logging_options = self._init_logging_options()
    
    def _detect_repeat_folders(self, dataset_dir):
        """
        Detect existing repeat folders in dataset directory
        Returns list of tuples: (folder_name, repeats, concept_name)
        """
        if not os.path.exists(dataset_dir):
            return []
            
        repeat_folders = []
        
        for item in os.listdir(dataset_dir):
            item_path = os.path.join(dataset_dir, item)
            if os.path.isdir(item_path):
                # Check if folder name matches repeat pattern
                try:
                    parts = item.split('_', 1)
                    if len(parts) >= 2:
                        repeats = int(parts[0])
                        concept_name = parts[1]
                        repeat_folders.append((item, repeats, concept_name))
                except ValueError:
                    # Not a repeat folder, skip
                    continue
                    
        return repeat_folders
    
    def _get_training_duration_config(self, config):
        """Generate training duration config based on user preference"""
        duration_mode = config.get('duration_mode', 'epochs')  # epochs or steps
        
        if duration_mode == 'steps':
            return {
                "max_train_steps": config.get('max_train_steps', 1600),
                "save_every_n_steps": config.get('save_every_n_steps', 400)
            }
        else:  # epochs mode (default)
            duration_config = {
                "max_train_epochs": config.get('epochs', config.get('max_train_epochs', 4)),
                "save_every_n_epochs": config.get('save_every_n_epochs', 1)
            }
            
            # Add steps if also specified (for partial epoch saves)
            if config.get('save_every_n_steps'):
                duration_config["save_every_n_steps"] = config['save_every_n_steps']
                
            return duration_config
    
    def _get_logging_config(self, config):
        """Generate logging configuration based on user preference"""
        logging_mode = config.get('logging_mode', 'none')
        
        if logging_mode not in self.logging_options:
            print(f"‚ö†Ô∏è Unknown logging mode '{logging_mode}', using 'none'")
            logging_mode = 'none'
        
        logging_config = {}
        logging_option = self.logging_options[logging_mode]
        
        print(f"üìä Logging mode: {logging_mode} - {logging_option['description']}")
        
        # Add basic log_with parameter
        if logging_mode == 'tensorboard':
            logging_config["log_with"] = "tensorboard"
        elif logging_mode == 'wandb':
            logging_config["log_with"] = "wandb"
            # Add WandB specific configs
            if config.get('wandb_api_key'):
                logging_config["wandb_api_key"] = config['wandb_api_key']
            if config.get('wandb_run_name'):
                logging_config["wandb_run_name"] = config['wandb_run_name']
        elif logging_mode == 'both':
            logging_config["log_with"] = "all"
            # Add WandB configs for both mode
            if config.get('wandb_api_key'):
                logging_config["wandb_api_key"] = config['wandb_api_key']
            if config.get('wandb_run_name'):
                logging_config["wandb_run_name"] = config['wandb_run_name']
        
        # Add common logging configs
        if config.get('log_prefix'):
            logging_config["log_prefix"] = config['log_prefix']
            
        return logging_config
    
    def _get_ip_noise_config(self, config):
        """Generate IP noise gamma configuration with 2024 randomization support"""
        ip_config = {}
        
        # Basic IP noise gamma
        if config.get('ip_noise_gamma_enabled', False):
            gamma_value = self._safe_float(config.get('ip_noise_gamma', 0.0))
            if gamma_value > 0:
                ip_config["ip_noise_gamma"] = gamma_value
                
                # 2024 randomization feature
                if config.get('ip_noise_gamma_random_strength', False):
                    ip_config["ip_noise_gamma_random_strength"] = True
                    print(f"üé≤ IP Noise Gamma: {gamma_value} (with randomization)")
                else:
                    print(f"üéØ IP Noise Gamma: {gamma_value}")
        
        return ip_config
    
    def _find_training_script(self):
        """Find the training script in various possible locations"""
        possible_scripts = [
            # Derrian's backend might have scripts in different locations
            os.path.join(self.trainer_dir, "derrian_backend", "sd_scripts", "sdxl_train_network.py"),
            os.path.join(self.trainer_dir, "derrian_backend", "sd_scripts", "train_network.py"), 
            os.path.join(self.trainer_dir, "sdxl_train_network.py"),
            os.path.join(self.trainer_dir, "train_network.py"),
            # Also check for kohya scripts structure
            os.path.join(self.sd_scripts_dir, "sdxl_train_network.py"),
            os.path.join(self.sd_scripts_dir, "train_network.py")
        ]
        
        for script_path in possible_scripts:
            if os.path.exists(script_path):
                script_type = "SDXL" if "sdxl" in os.path.basename(script_path) else "SD 1.5"
                print(f"‚úÖ Found {script_type} training script: {script_path}")
                return script_path
        
        return None
    
    def _setup_custom_optimizers(self):
        """Add paths for submodule components (silent mode)"""
        # Add LyCORIS to Python path
        lycoris_dir = os.path.join(self.trainer_dir, "derrian_backend", "lycoris")
        if os.path.exists(lycoris_dir) and lycoris_dir not in sys.path:
            sys.path.insert(0, lycoris_dir)
            
        # Add Derrian's custom optimizers to path  
        derrian_dir = os.path.join(self.trainer_dir, "derrian_backend")
        custom_scheduler_dir = os.path.join(derrian_dir, "custom_scheduler")
        if os.path.exists(custom_scheduler_dir) and custom_scheduler_dir not in sys.path:
            sys.path.insert(0, custom_scheduler_dir)
            
        # Silently check for custom optimizers
            
        # Try to import and verify CAME is available (from custom_scheduler)
        try:
            import LoraEasyCustomOptimizer.came
            from LoraEasyCustomOptimizer.came import CAME
            print("‚úÖ CAME optimizer found and ready!")
        except ImportError as e:
            print(f"‚ö†Ô∏è CAME optimizer not found: {e}")
            
            # Try alternative import paths for Derrian's structure
            alt_paths = [
                ('came', 'CAME'),
                ('optimizers.came', 'CAME'), 
                ('custom_optimizers.came', 'CAME'),
                ('custom_scheduler.LoraEasyCustomOptimizer.came', 'CAME')
            ]
            
            for module_path, class_name in alt_paths:
                try:
                    module = __import__(module_path, fromlist=[class_name])
                    getattr(module, class_name)  # Test if class exists
                    print(f"‚úÖ Found CAME at: {module_path}.{class_name}")
                    break
                except (ImportError, AttributeError):
                    continue
            else:
                print("üí° CAME optimizer not found in any expected location")
            
        # Try to import REX scheduler
        try:
            import LoraEasyCustomOptimizer.RexAnnealingWarmRestarts
            print("‚úÖ REX scheduler found and ready!")
        except ImportError as e:
            print(f"‚ö†Ô∏è REX scheduler not found: {e}")
            print("üí° Custom optimizers might not be available - using standard optimizers")
    
    def _test_triton_compatibility(self):
        """Test if Triton is working properly for AdamW8bit"""
        try:
            # Try to import and test basic Triton functionality
            import triton
            import torch
            
            # Test if we can create a simple operation that would use Triton
            # This mimics what bitsandbytes.AdamW8bit would try to do
            if not torch.cuda.is_available():
                print("üîç CUDA not available - AdamW8bit may not work optimally")
                return False
                
            # Try a simple tensor operation that would trigger Triton compilation
            test_tensor = torch.randn(10, device='cuda', dtype=torch.float16)
            _ = test_tensor * 2.0
            
            print("‚úÖ Triton compatibility test passed - AdamW8bit should work")
            return True
            
        except ImportError as e:
            print(f"‚ö†Ô∏è Triton not found: {e}")
            print("üí° Will fallback to AdamW for compatibility")
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è Triton compatibility test failed: {e}")
            print("üí° Will fallback to AdamW for compatibility")
            return False
    
    def _get_safe_optimizer_config(self, requested_optimizer):
        """Get optimizer config with automatic fallback for compatibility issues"""
        if requested_optimizer == 'AdamW8bit':
            # Test Triton compatibility before using AdamW8bit
            if not self._test_triton_compatibility():
                print("üîÑ Automatically switching from AdamW8bit to AdamW due to compatibility issues")
                print("üìã AdamW provides similar performance with broader compatibility")
                return 'AdamW'
        
        return requested_optimizer
    
    def _safe_float(self, value, default=0.0):
        """Safely convert value to float with fallback"""
        try:
            if isinstance(value, str):
                return float(value.strip()) if value.strip() else default
            return float(value) if value is not None else default
        except (ValueError, TypeError):
            return default
    
    def _get_logging_backend(self, config):
        """Determine the appropriate logging backend (wandb, tensorboard, or none)"""
        # Check if wandb key is provided and looks valid
        wandb_key = config.get('wandb_key', '').strip()
        if wandb_key and len(wandb_key) > 20:  # wandb keys are typically 40+ chars
            print("üìä Enabling WandB logging with provided API key")
            project_name = config.get('project_name', 'lora-training')
            print(f"üìä WandB project: {project_name}")
            return "wandb"
        elif wandb_key:
            print(f"‚ö†Ô∏è WandB API key too short ({len(wandb_key)} chars) - should be 40+ characters")
            print("üí° Get your API key from: https://wandb.ai/settings")
        elif config.get('wandb_key') == '':
            print("‚ÑπÔ∏è No WandB API key provided - skipping cloud logging")
        
        # Try tensorboard as fallback (usually works locally)
        try:
            import tensorboard
            print("üìä Using TensorBoard logging (local logs in training_outputs/)")
            return "tensorboard"
        except ImportError:
            print("üìä No logging backend available - training will run without metrics")
            print("üí° Install tensorboard: pip install tensorboard")
            return None
    
    def _calculate_warmup_steps(self, config):
        """Calculate warmup steps based on ACTUAL training steps, not hardcoded bias"""
        try:
            # Get actual training parameters
            dataset_size = config.get('dataset_size', 0)
            if dataset_size == 0:
                # Fallback: try to count images from dataset directory
                dataset_dir = config.get('dataset_dir', '')
                if dataset_dir:
                    from core.image_utils import count_images_in_directory
                    dataset_size = count_images_in_directory(dataset_dir)
            
            num_repeats = config.get('num_repeats', 1)
            epochs = config.get('epochs', 1)
            batch_size = config.get('train_batch_size', 1)
            warmup_ratio = self._safe_float(config.get('lr_warmup_ratio', 0.0))
            
            if dataset_size > 0 and batch_size > 0:
                # Calculate REAL total training steps
                total_steps = (dataset_size * num_repeats * epochs) // batch_size
                warmup_steps = int(total_steps * warmup_ratio)
                
                print(f"üî¢ UNBIASED WARMUP CALCULATION:")
                print(f"   üì∏ Dataset size: {dataset_size}")
                print(f"   üîÑ Repeats: {num_repeats}")
                print(f"   üìÖ Epochs: {epochs}")
                print(f"   üì¶ Batch size: {batch_size}")
                print(f"   ‚ö° Total steps: {total_steps}")
                print(f"   üå°Ô∏è Warmup ratio: {warmup_ratio}")
                print(f"   üî• Warmup steps: {warmup_steps}")
                
                return warmup_steps
            else:
                print("‚ö†Ô∏è Cannot calculate warmup steps - missing dataset info, using fallback")
                # Fallback: estimate reasonable total steps based on common dataset sizes
                estimated_dataset_size = 100  # Conservative estimate
                estimated_total_steps = (estimated_dataset_size * num_repeats * epochs) // batch_size
                warmup_steps = int(estimated_total_steps * warmup_ratio)
                
                print(f"üìä FALLBACK WARMUP CALCULATION:")
                print(f"   üì∏ Estimated dataset size: {estimated_dataset_size}")
                print(f"   ‚ö° Estimated total steps: {estimated_total_steps}")
                print(f"   üî• Fallback warmup steps: {warmup_steps}")
                
                return warmup_steps
                
        except Exception as e:
            print(f"‚ùå Error calculating warmup steps: {e}")
            # Final fallback - still use warmup ratio calculation
            epochs = config.get('epochs', 1)
            estimated_steps = max(epochs * 50, 100)  # Minimum reasonable steps
            warmup_ratio = self._safe_float(config.get('lr_warmup_ratio', 0.0))
            return int(estimated_steps * warmup_ratio)
    
    def _detect_model_type(self, config):
        """Detect model type from model path or config to choose appropriate SD scripts"""
        model_path = config.get('model_path', '').lower()
        
        # Check for Flux indicators
        if any(flux_indicator in model_path for flux_indicator in [
            'flux', 'schnell', 'dev', 'black-forest-labs'
        ]):
            return 'flux'
        
        # Check for SD3 indicators  
        if any(sd3_indicator in model_path for sd3_indicator in [
            'sd3', 'stable-diffusion-3', 'sd3-medium', 'sd3-large'
        ]):
            return 'sd3'
        
        # Check for SDXL indicators
        if any(sdxl_indicator in model_path for sdxl_indicator in [
            'sdxl', 'xl', '1024', 'base_1.0', 'refiner'
        ]):
            return 'sdxl'
        
        # Default to SD 1.5/2.x
        return 'sd15'
    
    
        
    def _init_advanced_optimizers(self) -> Dict[str, Dict[str, Any]]:
        """Initialize advanced optimizer configurations"""
        return {
            'came': {
                'optimizer_type': 'LoraEasyCustomOptimizer.came.CAME',
                'scheduler': 'LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts',
                'loss_type': 'huber',
                'huber_c': 0.1,
                'huber_schedule': 'snr',
                'args': ['weight_decay=0.04', 'eps=1e-16'],
                'scheduler_args': ['min_lr=1e-9', 'gamma=0.9', 'd=0.9'],
                'description': 'Derrian\'s memory-efficient optimizer'
            },
            'prodigy_plus': {
                'optimizer_type': 'ProdigyPlusScheduleFree',
                'scheduler': 'constant',  # Schedule-free
                'args': ['decouple=True', 'weight_decay=0.01', 'schedule_free=True'],
                'learning_rate_override': 1.0,
                'description': 'OneTrainer\'s learning rate AND schedule free'
            },
            'stable_adamw': {
                'optimizer_type': 'StableAdamW',
                'args': ['weight_decay=0.01', 'betas=[0.9,0.999]', 'eps=1e-8'],
                'description': 'Research-grade stability improvements'
            },
            'adopt': {
                'optimizer_type': 'ADOPT',
                'args': ['clip_threshold=1.0', 'eps=1e-6'],
                'description': 'Bleeding-edge adaptive gradient clipping'
            }
        }
    
    def _init_standard_optimizers(self) -> Dict[str, Dict[str, Any]]:
        """Initialize standard optimizer configurations"""
        return {
            'Prodigy': {
                'optimizer_type': 'Prodigy',  # Let SD scripts handle import
                'args': ['decouple=True', 'weight_decay=0.01', 'betas=[0.9,0.999]', 'd_coef=2', 'use_bias_correction=True'],
                'warmup_args': ['safeguard_warmup=True'],
                'description': 'Adaptive learning rate optimizer'
            },
            'AdamW': {
                'optimizer_type': 'AdamW',
                'args': ['weight_decay=0.01', 'betas=[0.9,0.999]'],
                'description': 'Standard AdamW optimizer'
            },
            'AdamW8bit': {
                'optimizer_type': 'bitsandbytes.optim.AdamW8bit',
                'args': ['weight_decay=0.1', 'betas=[0.9,0.99]'],
                'description': '8-bit AdamW for memory efficiency'
            },
            'AdaFactor': {
                'optimizer_type': 'transformers.optimization.Adafactor',
                'args': ['scale_parameter=False', 'relative_step=False', 'warmup_init=False'],
                'description': 'Memory-efficient AdaFactor optimizer'
            },
            'Came': {
                'optimizer_type': 'LoraEasyCustomOptimizer.came.CAME',
                'args': ['weight_decay=0.04'],
                'description': 'Custom CAME optimizer'
            },
            'DAdaptation': {
                'optimizer_type': 'DAdaptation',
                'args': ['weight_decay=0.01'],
                'description': 'D-Adaptation optimizer'
            },
            'DadaptAdam': {
                'optimizer_type': 'DadaptAdam',
                'args': ['weight_decay=0.01'],
                'description': 'D-Adaptation Adam variant'
            },
            'DadaptLion': {
                'optimizer_type': 'DadaptLion',
                'args': ['weight_decay=0.01'],
                'description': 'D-Adaptation Lion variant'
            },
            'Lion': {
                'optimizer_type': 'Lion',
                'args': ['weight_decay=0.01', 'betas=[0.9,0.99]'],
                'description': 'Lion optimizer'
            },
            'SGDNesterov': {
                'optimizer_type': 'SGDNesterov',
                'args': ['momentum=0.9', 'weight_decay=0.01'],
                'description': 'SGD with Nesterov momentum'
            },
            'SGDNesterov8bit': {
                'optimizer_type': 'bitsandbytes.optim.SGD8bit',
                'args': ['momentum=0.9', 'weight_decay=0.01'],
                'description': '8-bit SGD with Nesterov momentum'
            }
        }
    
    def _init_lycoris_methods(self) -> Dict[str, Dict[str, Any]]:
        """Initialize LyCORIS method configurations - OFFICIAL Algo-List.md specifications"""
        return {
            # DoRA removed - handled in main LoRA type selection
            'loha': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=loha'],
                'description': 'LoHa: Low-rank Hadamard (dim‚â§32, alpha‚â§dim, rank‚â§dim¬≤)',
                'recommendations': 'Recommended: dim‚â§32, alpha from 1 to dim. Higher dim may cause unstable loss.'
            },
            'lokr': {
                'network_module': 'lycoris.kohya', 
                'network_args': ['algo=lokr'],  # Removed factor, let LyCORIS handle
                'description': 'LoKr: Low-rank Kronecker (small/large LoKr configs available)',
                'recommendations': 'Can use "full dimension" by setting very large dimension.'
            },
            'ia3': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=ia3'],
                'description': '(IA)¬≥: Experimental method (requires higher learning rate, <1MB files)',
                'recommendations': 'Requires higher learning rate. Produces very small files (<1 MB).'
            },
            'dylora': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=dylora'],  # Removed block_size, use defaults
                'description': 'DyLoRA: Updates one row/column per step (large dim recommended)',
                'recommendations': 'Recommended: large dim with alpha=dim/4 to dim.'
            },
            'boft': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=boft'],
                'description': 'BOFT: Advanced Diag-OFT using butterfly operation for orthogonal matrix',
                'recommendations': 'Advanced version of Diag-OFT with butterfly operations.'
            },
            'glora': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=glora'],
                'description': 'GLoRA: Generalized LoRA (experimental)',
                'recommendations': 'Part of experimental methods, limited documentation available.'
            },
            'full': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=full'],
                'description': 'Full: Native fine-tuning (largest files, potentially best results)',
                'recommendations': 'Can potentially give the best result if tuned correctly. Largest file but no slower training.'
            },
            'diag_oft': {
                'network_module': 'lycoris.kohya',
                'network_args': ['algo=diag-oft'],  # Fixed: uses hyphen not underscore!
                'description': 'Diag-OFT: Preserves hyperspherical energy (converges faster than LoRA)',
                'recommendations': 'Preserves hyperspherical energy. Converges faster than LoRA.'
            }
            # Note: norms not mentioned in official Algo-List.md
        }
        
    def _init_experimental_features(self) -> Dict[str, Dict[str, Any]]:
        """Initialize experimental feature configurations"""
        return {
            'fused_back_pass': {
                'enabled': False,
                'description': 'OneTrainer memory optimization',
                'requirements': ['compatible_optimizer', 'no_gradient_accumulation']
            },
            'eq_vae': {
                'enabled': False,
                'description': 'HakuLatent Equivariance Regularization',
                'status': 'future_implementation'
            },
            'adversarial_loss': {
                'enabled': False,
                'description': 'GAN-style training improvements', 
                'status': 'research_grade'
            },
            'multi_resolution': {
                'enabled': False,
                'description': 'Train on multiple resolutions simultaneously',
                'status': 'experimental'
            }
        }
    
    def _init_logging_options(self):
        """Initialize logging system options"""
        return {
            'none': {
                'description': 'No logging (standard console output only)',
                'args': []
            },
            'tensorboard': {
                'description': 'TensorBoard logging for training metrics',
                'args': ['--log_with', 'tensorboard']
            },
            'wandb': {
                'description': 'Weights & Biases logging for experiment tracking',
                'args': ['--log_with', 'wandb']
            },
            'both': {
                'description': 'Both TensorBoard and WandB logging',
                'args': ['--log_with', 'all']
            }
        }

    def _create_dataset_toml(self, config):
        """Create dataset TOML with support for repeat folders"""
        
        dataset_config = {
            "general": {
                "resolution": config['resolution'],
                "shuffle_caption": config.get('shuffle_caption', True),
                "keep_tokens": config.get('keep_tokens', 0),
                "flip_aug": config['flip_aug'],
                "caption_extension": ".txt",
                "enable_bucket": config.get('enable_bucket', True),
                "bucket_no_upscale": config.get('bucket_no_upscale', False),
                "bucket_reso_steps": config.get('bucket_reso_steps', 64),
                "min_bucket_reso": config.get('min_bucket_reso', 256),
                "max_bucket_reso": config.get('max_bucket_reso', 2048),
                "caption_dropout_rate": config.get('caption_dropout_rate', 0.0),
                "caption_tag_dropout_rate": config.get('caption_tag_dropout_rate', 0.0),
            }
        }
        
        # Check for repeat folders in the dataset directory
        dataset_dir = config['dataset_dir']
        repeat_folders = self._detect_repeat_folders(dataset_dir)
        
        if repeat_folders:
            # Use repeat folders - sd_scripts will auto-detect repeats from folder names
            print(f"üîÑ Detected {len(repeat_folders)} repeat folder(s), using automatic repeat detection")
            subsets = []
            total_effective_images = 0
            
            for folder_name, repeats, concept_name in repeat_folders:
                folder_path = os.path.join(os.path.abspath(dataset_dir), folder_name)
                subsets.append({
                    "image_dir": folder_path,
                    # Don't specify num_repeats - let sd_scripts auto-detect from folder name
                })
                
                # Calculate effective images for reporting
                image_count = len([f for f in os.listdir(folder_path) 
                                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))])
                total_effective_images += image_count * repeats
                print(f"  üìÇ {folder_name}: {image_count} images √ó {repeats} repeats = {image_count * repeats} per epoch")
            
            print(f"‚úÖ Total effective training images per epoch: {total_effective_images}")
            
        else:
            # Standard single folder approach - use num_repeats from config
            print("üìÅ Using standard folder structure with TOML-configured repeats")
            subsets = [{
                "num_repeats": config['num_repeats'],
                "image_dir": os.path.abspath(dataset_dir),
            }]
        
        dataset_config["datasets"] = [{"subsets": subsets}]
        
        dataset_toml_path = os.path.join(self.config_dir, "dataset.toml")
        with open(dataset_toml_path, "w") as f:
            toml.dump(dataset_config, f)
        return dataset_toml_path

    def _create_config_toml(self, config):
        """üîß Enhanced config generation with hybrid features"""
        
        # ü¶Ñ LyCORIS Method Selection
        network_args = []
        network_module = "networks.lora"  # Default
        
        # Check for advanced LyCORIS methods
        lycoris_method = config.get('lycoris_method', 'none')
        if lycoris_method != 'none' and lycoris_method in self.lycoris_methods:
            lycoris_config = self.lycoris_methods[lycoris_method]
            network_module = lycoris_config['network_module']
            network_args = lycoris_config['network_args'].copy()
            
            # Add official LyCORIS network arguments from Network-Args.md
            
            # Add preset configuration (official presets: full, attn-mlp, attn-only, etc.)
            preset = config.get('lycoris_preset', 'attn-mlp')  # Default per Guidelines.md
            if preset != 'full':  # full is default, no need to specify
                network_args.append(f"preset={preset}")
            
            # Add dimension args if applicable
            if 'conv_dim' in config and config['conv_dim'] > 0:
                network_args.append(f"conv_dim={config['conv_dim']}")
            if 'conv_alpha' in config and config['conv_alpha'] > 0:
                network_args.append(f"conv_alpha={config['conv_alpha']}")
                
            # Add dropout options if specified
            if config.get('lycoris_dropout', 0) > 0:
                network_args.append(f"dropout={config['lycoris_dropout']}")
            if config.get('lycoris_rank_dropout', 0) > 0:
                network_args.append(f"rank_dropout={config['lycoris_rank_dropout']}")
            if config.get('lycoris_module_dropout', 0) > 0:
                network_args.append(f"module_dropout={config['lycoris_module_dropout']}")
                
            # Add advanced options if enabled
            if config.get('lycoris_use_tucker', False):
                network_args.append("use_tucker=True")
            if config.get('lycoris_use_scalar', False):
                network_args.append("use_scalar=True")
            if config.get('lycoris_train_norm', False):
                network_args.append("train_norm=True")
            if config.get('lycoris_bypass_mode', False):
                network_args.append("bypass_mode=True")
                
            # Algorithm-specific parameters
            if lycoris_method == 'lokr' and config.get('lycoris_factor', 0) != 0:
                network_args.append(f"factor={config['lycoris_factor']}")
            
            # DoRA weight decomposition (can be used with most algorithms)
            if config.get('lycoris_dora_wd', False):
                network_args.append("dora_wd=True")
                
            print(f"ü¶Ñ Using LyCORIS method: {lycoris_method} - {lycoris_config['description']}")
            if preset != 'attn-mlp':
                print(f"üìã Preset: {preset}")
            if config.get('lycoris_dropout', 0) > 0:
                print(f"üíß Dropout: {config['lycoris_dropout']}")
        
        # Fallback to basic LoRA types (updated for latest Kohya-SS)
        elif config['lora_type'] == "LoRA":
            network_module = "networks.lora"  # Standard LoRA
            network_args = []
            # Add LoRA+ support if enabled
            if config.get('enable_loraplus', False):
                loraplus_ratio = config.get('loraplus_lr_ratio', 16)
                network_args.append(f"loraplus_lr_ratio={loraplus_ratio}")
                print(f"‚ú® Using LoRA+ with ratio {loraplus_ratio}")
        elif config['lora_type'] == "LoCon":
            network_module = "lycoris.kohya"
            network_args = [f"algo=locon"]  # Official docs show locon, not lora
            # Add conv dimensions if specified (LoCon supports conv layers)
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
            print("üîß Using LoCon - LoRA with Convolution support (official: dim‚â§64, alpha‚â§dim/2)")
        elif config['lora_type'] == "DoRA (Weight Decomposition)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=lora", f"dora_wd=True"]
            # Add conv dimensions for DoRA if specified
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
            print("üéØ Using DoRA - expect 2-3x slower training but higher quality!")
        elif config['lora_type'] == "LoHa (Hadamard Product)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=loha"]
            # LoHa can use conv dimensions but they're optional
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
        elif config['lora_type'] == "LoKr (Kronecker Product)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=lokr"]
            # LoKr can use conv dimensions but they're optional
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
        elif config['lora_type'] == "(IA)¬≥ (Few Parameters)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=ia3"]  # IA3 uses learnable scalar scaling, no conv dimensions needed
        elif config['lora_type'] == "DyLoRA":
            network_module = "lycoris.kohya"
            network_args = [f"algo=dylora"]
            # DyLoRA can use conv dimensions
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
        elif config['lora_type'] == "BOFT (Butterfly Transform)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=boft"]
            # BOFT can use conv dimensions
            if config.get('conv_dim', 0) > 0:
                network_args.extend([f"conv_dim={config['conv_dim']}", f"conv_alpha={config.get('conv_alpha', 4)}"])
        elif config['lora_type'] == "GLoRA (Generalized LoRA)":
            network_module = "lycoris.kohya"
            network_args = [f"algo=glora"]
            # GLoRA: Does NOT use conv_dim/conv_alpha - it extracts dimensions from original modules
            # Uses different weight structure (a1,a2,b1,b2) instead of lora_down/lora_up
            print("üåü Using GLoRA - Generalized LoRA with a1/a2/b1/b2 weight structure!")
            print("‚ÑπÔ∏è Note: GLoRA automatically detects layer dimensions (no conv_dim/conv_alpha needed)")

        # üöÄ Advanced Optimizer Handling
        optimizer_args = []
        requested_optimizer = config['optimizer']  # Default to widget selection
        optimizer_type = self._get_safe_optimizer_config(requested_optimizer)  # Apply fallback logic
        lr_scheduler_type = None
        lr_scheduler_args = None
        loss_type = None
        huber_c = None
        huber_schedule = None
        
        # Check for advanced optimizer
        advanced_optimizer = config.get('advanced_optimizer', 'standard')
        if advanced_optimizer != 'standard' and advanced_optimizer in self.advanced_optimizers:
            adv_config = self.advanced_optimizers[advanced_optimizer]
            optimizer_type = adv_config['optimizer_type']
            optimizer_args = adv_config['args'].copy()
            
            # Auto-configure scheduler if specified
            if 'scheduler' in adv_config:
                lr_scheduler_type = adv_config['scheduler']
            if 'scheduler_args' in adv_config:
                lr_scheduler_args = adv_config['scheduler_args']
            if 'loss_type' in adv_config:
                loss_type = adv_config['loss_type']
                print(f"üéØ Loss Function: {loss_type} (more robust than MSE)")
            if 'huber_c' in adv_config:
                huber_c = adv_config['huber_c'] 
                print(f"üìä Huber C: {huber_c} (threshold parameter)")
            if 'huber_schedule' in adv_config:
                huber_schedule = adv_config['huber_schedule']
                print(f"üìà Huber Schedule: {huber_schedule} (SNR-based scheduling)")
            
            print(f"üöÄ Using advanced optimizer: {advanced_optimizer} - {adv_config['description']}")
            
            # Check if CAME optimizer is actually available
            if advanced_optimizer == 'came':
                try:
                    import LoraEasyCustomOptimizer.came
                    print("‚úÖ CAME optimizer verified and ready!")
                except ImportError as e:
                    print(f"‚ùå CAME optimizer not available: {e}")
                    print("üîÑ Falling back to standard AdamW optimizer...")
                    optimizer_type = "AdamW"
                    optimizer_args = ["weight_decay=0.01", "betas=[0.9,0.999]"]
                    lr_scheduler_type = None
                    lr_scheduler_args = None
                    advanced_optimizer = 'standard'  # Reset to prevent other CAME-specific logic
            
            # Override learning rate if specified (and optimizer is still advanced)
            if advanced_optimizer != 'standard' and 'learning_rate_override' in adv_config:
                config['unet_lr'] = adv_config['learning_rate_override']
                config['text_encoder_lr'] = adv_config['learning_rate_override']
                print(f"üìä Learning rate auto-set to: {adv_config['learning_rate_override']}")
        
        # Standard optimizer configurations - use dictionary lookup for cleaner code
        elif optimizer_type in self.standard_optimizers:
            std_config = self.standard_optimizers[optimizer_type]
            optimizer_type = std_config['optimizer_type']
            optimizer_args.extend(std_config['args'])
            
            # Handle special cases like Prodigy warmup
            if optimizer_type == 'Prodigy' and config.get('lr_warmup_ratio', 0) > 0:
                if 'warmup_args' in std_config:
                    optimizer_args.extend(std_config['warmup_args'])
            
            print(f"üîß Using {optimizer_type}: {std_config['description']}")
        
        # If no match, use the optimizer name as-is and let SD scripts handle it
        else:
            print(f"‚ö†Ô∏è Unknown optimizer '{config['optimizer']}' - using as-is")

        # Handle REX scheduler
        if config['lr_scheduler'] == "rex" and not lr_scheduler_type:
            try:
                import LoraEasyCustomOptimizer.RexAnnealingWarmRestarts
                lr_scheduler_type = "LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts"
                lr_scheduler_args = ["min_lr=1e-9", "gamma=0.9", "d=0.9"]
                print("‚úÖ REX scheduler verified and ready!")
            except ImportError as e:
                print(f"‚ùå REX scheduler not available: {e}")
                print("üîÑ Falling back to cosine scheduler...")
                lr_scheduler_type = None  # Use default cosine
                lr_scheduler_args = None

        training_args = {
            "lowram": True,
            "pretrained_model_name_or_path": os.path.abspath(config['model_path']),
            # Training duration - support both epochs and steps
            **self._get_training_duration_config(config),
            "train_batch_size": config['train_batch_size'],
            "mixed_precision": config['precision'],
            "save_precision": config['precision'],
            "save_every_n_epochs": config['save_every_n_epochs'],
            "save_last_n_epochs": config['keep_only_last_n_epochs'],
            "output_name": config['project_name'],
            "output_dir": self.output_dir,
            "logging_dir": self.logging_dir,
            "cache_latents": config['cache_latents'],
            "cache_latents_to_disk": config['cache_latents_to_disk'], 
            "cache_text_encoder_outputs": config['cache_text_encoder_outputs'],
            "cache_text_encoder_outputs_to_disk": config.get('cache_text_encoder_outputs_to_disk', False),
            "min_snr_gamma": self._safe_float(config['min_snr_gamma']) if config['min_snr_gamma_enabled'] else None,
            # Enhanced IP noise gamma support (2024 randomization)
            **self._get_ip_noise_config(config),
            "multires_noise_iterations": config.get('multires_noise_iterations', 6) if config['multinoise'] else None,
            "multires_noise_discount": config.get('multires_noise_discount', 0.3) if config['multinoise'] else None,
            "xformers": config['cross_attention'] == "xformers",
            "sdpa": config['cross_attention'] == "sdpa",
            # Enhanced logging system
            **self._get_logging_config(config),
            # Advanced training options
            "noise_offset": self._safe_float(config.get('noise_offset', 0.0)) if self._safe_float(config.get('noise_offset', 0.0)) > 0 else None,
            "adaptive_noise_scale": self._safe_float(config.get('adaptive_noise_scale', 0.0)) if self._safe_float(config.get('adaptive_noise_scale', 0.0)) > 0 else None,
            "zero_terminal_snr": config.get('zero_terminal_snr', False),
            "clip_skip": config.get('clip_skip', 2),
            "vae_batch_size": config.get('vae_batch_size', 1),
            "no_half_vae": config.get('no_half_vae', False),
            # User-configurable training flags (no more hardcoding!)
            "gradient_checkpointing": config.get('gradient_checkpointing', True),
            "gradient_accumulation_steps": config.get('gradient_accumulation_steps', 1),
            "max_grad_norm": float(config.get('max_grad_norm', 1.0)),
            "full_fp16": config.get('full_fp16', False),
            "random_crop": config.get('random_crop', False),
            "fp8_base": config.get('fp8_base', False),
        }

        # Add v_parameterization only if explicitly enabled
        if config.get('v2', False):
            training_args["v2"] = True
            print("‚úÖ SD 2.x mode enabled for SD 2.0/2.1 base models")
        
        if config.get('v_parameterization', False):
            training_args["v_parameterization"] = True
            print("‚úÖ V-Parameterization enabled for v-pred models")
        
        # Handle text encoder training vs caching logic
        if config.get('cache_text_encoder_outputs', False):
            # Auto-disable text encoder training when caching is enabled
            training_args["network_train_unet_only"] = True
            print("üéØ Text encoder caching enabled - automatically disabling text encoder training")
        elif config.get('network_train_unet_only', False):
            training_args["network_train_unet_only"] = True
            print("üéØ Training U-Net only (SDXL LoRA optimization)")

        # Apply experimental features
        training_args = self._apply_experimental_features(config, training_args)

        config_toml = {
            "network_arguments": {
                "unet_lr": config['unet_lr'],
                "text_encoder_lr": config['text_encoder_lr'],
                "network_dim": config['network_dim'],
                "network_alpha": config['network_alpha'],
                "network_module": network_module,
                "network_args": network_args,
                "network_weights": config['continue_from_lora'] if config['continue_from_lora'] else None,
            },
            "optimizer_arguments": {
                "learning_rate": config['unet_lr'],
                "lr_scheduler": config['lr_scheduler'],
                "lr_scheduler_num_cycles": config['lr_scheduler_number'] if config['lr_scheduler'] == "cosine_with_restarts" else None,
                "lr_warmup_steps": self._calculate_warmup_steps(config) if self._safe_float(config['lr_warmup_ratio']) > 0 else None, # Proper calculation based on actual training steps
                "optimizer_type": optimizer_type,
                "optimizer_args": optimizer_args if optimizer_args else None,
                "lr_scheduler_type": lr_scheduler_type,
                "lr_scheduler_args": lr_scheduler_args,
                "loss_type": loss_type if loss_type else None,
                "huber_c": huber_c if huber_c else None,
                "huber_schedule": huber_schedule if huber_schedule else None,
            },
            "training_arguments": training_args
        }
        config_toml_path = os.path.join(self.config_dir, "config.toml")
        with open(config_toml_path, "w") as f:
            toml.dump(config_toml, f)
        return config_toml_path

    def _validate_advanced_config(self, config) -> Dict[str, any]:
        """üõ°Ô∏è Validate advanced configuration for compatibility issues (pure function)"""
        validation_result = {
            'warnings': [],
            'suggested_changes': {},
            'conflicts': []
        }
        
        # Check memory requirements and auto-adjust for VRAM constraints
        batch_size = config.get('train_batch_size', 1)
        resolution = config.get('resolution', 1024)
        
        # VRAM usage estimation (very rough)
        estimated_vram_gb = (batch_size * resolution * resolution) / (1024 * 1024 * 200)  # Rough estimate
        
        if estimated_vram_gb > 20:  # Likely too much for most cards
            new_batch_size = max(1, batch_size // 2)
            validation_result['warnings'].append(f"üíæ High VRAM usage detected")
            validation_result['suggested_changes']['train_batch_size'] = new_batch_size
            
        # Auto-enable basic memory saving for large models (but respect user choices)
        if resolution >= 1024:
            validation_result['warnings'].append("üíæ Large resolution detected - enabling basic memory optimizations")
            if not config.get('cache_latents'):
                validation_result['suggested_changes']['cache_latents'] = True
                validation_result['warnings'].append("‚ÑπÔ∏è Auto-enabled latent caching for memory savings")
            if not config.get('cache_latents_to_disk'):
                validation_result['suggested_changes']['cache_latents_to_disk'] = True
                validation_result['warnings'].append("‚ÑπÔ∏è Auto-enabled disk caching for memory savings")
        
        # Validate text encoder conflicts (but don't auto-fix)
        if config.get('cache_text_encoder_outputs') and config.get('shuffle_caption'):
            validation_result['conflicts'].append("üö® CONFLICT: Cannot use caption shuffling with text encoder caching!")
            validation_result['conflicts'].append("üí° Please disable one of these options in the training widget")
            
        if config.get('cache_text_encoder_outputs') and config.get('text_encoder_lr', 0) > 0:
            validation_result['conflicts'].append("üö® CONFLICT: Cannot cache text encoder while training it!")
            validation_result['conflicts'].append("üí° Set Text Encoder LR to 0 or disable text encoder caching")
        
        # Check fused back pass compatibility
        if config.get('fused_back_pass', False):
            if config.get('train_batch_size', 1) > 1:
                validation_result['warnings'].append("‚ö†Ô∏è Fused Back Pass requires gradient accumulation = 1 (batch size 1)")
                validation_result['suggested_changes']['train_batch_size'] = 1
                
            advanced_optimizer = config.get('advanced_optimizer', 'standard')
            if advanced_optimizer not in ['came', 'prodigy_plus', 'stable_adamw']:
                validation_result['warnings'].append("‚ö†Ô∏è Fused Back Pass may not be compatible with this optimizer")
        
        # Check DoRA training time warning
        if config.get('lycoris_method') == 'dora':
            validation_result['warnings'].append("üêå DoRA training is 2-3x slower but higher quality - be patient!")
        
        # Check CAME + REX pairing
        advanced_optimizer = config.get('advanced_optimizer', 'standard') 
        if advanced_optimizer == 'came' and config.get('lr_scheduler') != 'rex':
            validation_result['warnings'].append("üí° CAME optimizer works best with REX scheduler - auto-switching")
            validation_result['suggested_changes']['lr_scheduler'] = 'rex'
        
        # Check Prodigy learning rate
        if advanced_optimizer == 'prodigy_plus':
            if config.get('unet_lr', 1.0) != 1.0:
                validation_result['warnings'].append("üìä Prodigy Plus is learning rate free - setting LR to 1.0")
                validation_result['suggested_changes']['unet_lr'] = 1.0
                validation_result['suggested_changes']['text_encoder_lr'] = 1.0
        
        return validation_result
    
    def _run_derrian_validation(self, config) -> Dict[str, any]:
        """üîç Run Derrian's comprehensive validation system"""
        validation_result = {
            'passed': False,
            'is_sdxl': False,
            'errors': [],
            'warnings': [],
            'suggestions': {}
        }
        
        try:
            # Add derrian_backend to path if not already there
            # NOTE: Using relative paths for cross-platform compatibility (Windows/Mac/Linux)
            derrian_dir = os.path.join(self.trainer_dir, "derrian_backend")
            if derrian_dir not in sys.path:
                sys.path.insert(0, derrian_dir)
            
            from utils import validation
            
            # Convert our config format to Derrian's expected format
            derrian_config = self._convert_config_to_derrian_format(config)
            
            # Run Derrian's validation
            passed, is_sdxl, errors, args_data, dataset_data, tag_data = validation.validate(derrian_config)
            
            validation_result.update({
                'passed': passed,
                'is_sdxl': is_sdxl,
                'errors': errors,
                'args_data': args_data,
                'dataset_data': dataset_data,
                'tag_data': tag_data
            })
            
            if passed:
                print("‚úÖ Derrian's validation passed - configuration looks good!")
            else:
                print("‚ö†Ô∏è Derrian's validation found issues:")
                for error in errors:
                    print(f"   ‚ùå {error}")
                    validation_result['warnings'].append(f"Derrian validation: {error}")
            
            if is_sdxl:
                print("üéØ SDXL model detected - optimizing configuration")
                validation_result['suggestions']['sdxl_optimized'] = True
                
        except ImportError as e:
            print(f"‚ö†Ô∏è Derrian's validation not available: {e}")
            validation_result['warnings'].append("Derrian's validation system not available - using basic validation only")
        except Exception as e:
            print(f"‚ö†Ô∏è Error running Derrian's validation: {e}")
            validation_result['warnings'].append(f"Derrian validation error: {e}")
        
        return validation_result
    
    def _convert_config_to_derrian_format(self, config) -> Dict[str, any]:
        """Convert our widget config format to Derrian's validation format"""
        # This is a simplified conversion - may need expansion based on testing
        derrian_config = {
            "args": {
                "basic": {
                    "pretrained_model_name_or_path": config.get('model_path', ''),
                    "output_dir": config.get('output_dir', './output'),
                    "max_train_epochs": config.get('epochs', 10),
                    "train_batch_size": config.get('train_batch_size', 1),
                    "learning_rate": config.get('unet_lr', 1e-4),
                    "text_encoder_lr": config.get('text_encoder_lr', 1e-5),
                    "network_dim": config.get('network_dim', 32),
                    "network_alpha": config.get('network_alpha', 16),
                    "optimizer_type": config.get('optimizer', 'AdamW8bit'),
                    "lr_scheduler_type": config.get('lr_scheduler', 'cosine'),
                    "mixed_precision": config.get('precision', 'fp16'),
                    "resolution": config.get('resolution', 1024)
                }
            },
            "dataset": {
                "general": {
                    "resolution": config.get('resolution', 1024),
                    "batch_size": config.get('train_batch_size', 1),
                    "enable_bucket": config.get('enable_bucket', True),
                    "min_bucket_reso": 256,
                    "max_bucket_reso": 2048,
                    "bucket_reso_steps": 64,
                    "caption_extension": ".txt"
                },
                "subsets": [
                    {
                        "image_dir": config.get('train_data_dir', ''),
                        "num_repeats": config.get('num_repeats', 1)
                    }
                ]
            }
        }
        
        return derrian_config
    
    def _apply_experimental_features(self, config, training_args: Dict[str, Any]) -> Dict[str, Any]:
        """üî¨ Apply experimental features to training arguments"""
        
        # Fused Back Pass (OneTrainer) - Currently disabled, requires OneTrainer backend
        if config.get('fused_back_pass', False):
            print("‚ö†Ô∏è Fused Back Pass requires OneTrainer integration - feature disabled")
            print("üí° Using standard gradient checkpointing for VRAM optimization instead")
        
        # ‚ö†Ô∏è IMPORTANT: DO NOT HARDCODE TRAINING ARGUMENTS HERE!
        # All training settings must come from user widgets, not manager assumptions.
        # If you hardcode values here, users can't control their own training!
        
        # Only add features that are explicitly enabled by the user
        # No auto-enabling of experimental features
        
        return training_args

    def launch_from_files(self, config_paths, monitor_widget=None):
        """üöÄ Launch training from existing TOML files - DEAD SIMPLE APPROACH"""
        
        print("üîç FRANKENSTEIN TRAINING MANAGER - FILE HUNTING MODE! üí•")
        print(f"üìÅ Using config files: {list(config_paths.keys())}")
        
        # Extract the file paths
        config_toml_path = config_paths.get("config.toml")
        dataset_toml_path = config_paths.get("dataset.toml") 
        
        if not config_toml_path or not dataset_toml_path:
            print("‚ùå Missing required TOML files!")
            return False
        
        print(f"‚úÖ Config TOML: {config_toml_path}")
        print(f"‚úÖ Dataset TOML: {dataset_toml_path}")
        
        # Find the training script
        train_script = self._find_training_script()
        if not train_script:
            print("‚ùå No training script found!")
            return False
        
        # Check for venv python
        # Cross-platform venv path
        if sys.platform == "win32":
            venv_python_path = os.path.join(self.sd_scripts_dir, "venv", "Scripts", "python.exe")
        else:
            venv_python_path = os.path.join(self.sd_scripts_dir, "venv", "bin", "python")
        if os.path.exists(venv_python_path):
            venv_python = venv_python_path
        else:
            venv_python = "python"
        
        print("\nüöÄ Starting training from TOML files...")
        
        # Set memory optimization environment variables
        env = os.environ.copy()
        env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        env['CUDA_LAUNCH_BLOCKING'] = '1'
        
        try:
            # Set up training monitoring
            if monitor_widget:
                monitor_widget.clear_log()
                monitor_widget.update_phase("Launching training from files...", "info")
            
            # Change to scripts directory for proper LyCORIS imports (like Holo's approach)
            original_cwd = os.getcwd()
            scripts_dir = self.sd_scripts_dir if os.path.exists(self.sd_scripts_dir) else self.trainer_dir
            os.chdir(scripts_dir)
            
            self.process = subprocess.Popen(
                [venv_python, train_script,
                 "--config_file", config_toml_path,
                 "--dataset_config", dataset_toml_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env
            )

            for line in iter(self.process.stdout.readline, ''):
                print(line, end='')
                
                # Update monitor widget if provided
                if monitor_widget:
                    monitor_widget.parse_training_output(line)
            
            self.process.stdout.close()
            return_code = self.process.wait()

            if return_code:
                raise subprocess.CalledProcessError(return_code, [venv_python, train_script])

            print("\n‚úÖ TRAINING COMPLETE! üéâ")
            return True

        except subprocess.CalledProcessError as e:
            print(f"üí• Training error occurred: {e}")
            return False
        except Exception as e:
            print(f"üö® Unexpected error: {e}")
            return False
        finally:
            # Restore original working directory
            os.chdir(original_cwd)
            self.process = None

    def stop_training():
        """üõë Terminate the training process"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            self.process.wait()
            print("üõë Training process terminated.")
            self.process = None

    def prepare_config_only(self, config):
        """üõ†Ô∏è Generate TOML files without starting training"""
        print("üìù Generating configuration files...")
        
        dataset_toml_path = self._create_dataset_toml(config)
        config_toml_path = self._create_config_toml(config)
        
        print(f"‚úÖ Generated: {dataset_toml_path}")
        print(f"‚úÖ Generated: {config_toml_path}")
        
        return {"config.toml": config_toml_path, "dataset.toml": dataset_toml_path}

    def start_training(self, config, monitor_widget=None):
        """üöÄ Start hybrid training with all advanced features"""
        
        print("üß™ FRANKENSTEIN TRAINING MANAGER ACTIVATED! üí•")
        print("Preparing hybrid training configuration...")
        
        # Validate advanced configuration (pure function - no side effects)
        validation = self._validate_advanced_config(config)
        
        # Enhanced validation using Derrian's sophisticated validation system
        derrian_validation = self._run_derrian_validation(config)
        
        # Apply suggested changes to config
        for key, value in validation['suggested_changes'].items():
            config[key] = value
            
        # Apply Derrian validation suggestions
        if derrian_validation['passed'] and 'suggestions' in derrian_validation:
            for key, value in derrian_validation['suggestions'].items():
                if key == 'sdxl_optimized' and value:
                    print("üéØ Applying SDXL optimizations from Derrian's validation")
                    # SDXL optimizations could be applied here
            
        # Print all validation results
        for warning in validation['warnings']:
            print(warning)
        for conflict in validation['conflicts']:
            print(conflict)
        for warning in derrian_validation.get('warnings', []):
            print(warning)
        
        # Show advanced features summary
        if config.get('advanced_mode_enabled', False):
            self._print_advanced_features_summary(config)
        
        dataset_toml_path = self._create_dataset_toml(config)
        config_toml_path = self._create_config_toml(config)

        # Check for venv python first, fall back to system python (cross-platform)
        if sys.platform == "win32":
            venv_python_path = os.path.join(self.sd_scripts_dir, "venv", "Scripts", "python.exe")
        else:
            venv_python_path = os.path.join(self.sd_scripts_dir, "venv", "bin", "python")
            
        if os.path.exists(venv_python_path):
            venv_python = venv_python_path
        else:
            venv_python = "python"  # Use system python (common in containers)
        
        # Find the training script in flexible locations
        train_script = self._find_training_script()
        if not train_script:
            print("‚ùå No training script found in any expected location!")
            print("üí° Please ensure the environment setup completed successfully!")
            return

        print("\nüöÄ Starting hybrid training...")
        
        # Set memory optimization environment variables
        env = os.environ.copy()
        env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Reduce fragmentation
        env['CUDA_LAUNCH_BLOCKING'] = '1'  # Better error reporting
        print("üíæ Memory optimization: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
        
        try:
            # Set up training monitoring
            if monitor_widget:
                monitor_widget.clear_log()
                monitor_widget.update_phase("Initializing training process...", "info")
                # Set total epochs for progress tracking
                monitor_widget.total_epochs = config['epochs']
            
            # Change to scripts directory for proper LyCORIS imports (like Holo's approach)
            original_cwd = os.getcwd()
            scripts_dir = self.sd_scripts_dir if os.path.exists(self.sd_scripts_dir) else self.trainer_dir
            os.chdir(scripts_dir)
            
            self.process = subprocess.Popen(
                [venv_python, train_script,
                 "--config_file", config_toml_path,
                 "--dataset_config", dataset_toml_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                env=env
            )

            for line in iter(self.process.stdout.readline, ''):
                print(line, end='')  # Still print to console
                
                # Update monitor widget if provided
                if monitor_widget:
                    monitor_widget.parse_training_output(line)
            
            self.process.stdout.close()
            return_code = self.process.wait()

            if return_code:
                raise subprocess.CalledProcessError(return_code, [venv_python, train_script])

            print("\n‚úÖ FRANKENSTEIN TRAINING COMPLETE! üéâ")
            print("Either it worked perfectly or it blew up spectacularly! üòÑ")

        except subprocess.CalledProcessError as e:
            print(f"üí• Training error occurred: {e}")
            print("\nüîß TROUBLESHOOTING SUGGESTIONS:")
            if "CAME" in str(e) or "LoraEasyCustomOptimizer" in str(e):
                print("1. üß™ CAME/REX optimizer issue - custom optimizers may not be installed")
                print("2. üîÑ Try disabling Advanced Mode and using standard AdamW optimizer")
                print("3. üì¶ Run environment setup again to install custom optimizers")
                print("4. üöÄ Switch to Prodigy optimizer as alternative to CAME")
            elif "bitsandbytes" in str(e) or "triton" in str(e):
                print("1. üìä Try switching from AdamW8bit to AdamW (compatibility issues)")
                print("2. üîß bitsandbytes/triton version conflict detected")
                print("3. üöÄ Use standard optimizers: AdamW, Prodigy, Lion")
            else:
                print("1. üíæ Reduce batch size if getting CUDA out of memory")
                print("2. üéØ Check that model path and dataset directory exist")
                print("3. üìä Try different optimizer (AdamW, Prodigy)")
                print("4. üîß Check file paths and permissions")
            print("5. üìù Check the detailed error log above for specific issues")
            return False
        except Exception as e:
            print(f"üö® Unexpected error: {e}")
            print("üß™ Welcome to the bleeding edge - backup and try again!")
            return False
        finally:
            # Restore original working directory
            os.chdir(original_cwd)
            self.process = None
    
    def _print_advanced_features_summary(self, config):
        """üìä Print summary of active advanced features"""
        print("\nüî¨ ADVANCED FEATURES ACTIVE:")
        
        advanced_optimizer = config.get('advanced_optimizer', 'standard')
        if advanced_optimizer != 'standard':
            print(f"  üöÄ Optimizer: {advanced_optimizer}")
        
        lycoris_method = config.get('lycoris_method', 'none')
        if lycoris_method != 'none':
            print(f"  ü¶Ñ LyCORIS Method: {lycoris_method}")
        
        if config.get('fused_back_pass', False):
            print(f"  ‚ö° Fused Back Pass: Enabled (VRAM optimization)")
        
        experimental_features = config.get('experimental_features', {})
        active_experiments = [k for k, v in experimental_features.items() if v]
        if active_experiments:
            print(f"  üî¨ Experimental: {len(active_experiments)} features enabled")
        
        print("  üíÄ No guarantees - you asked for this! üòà\n")


# Backwards compatibility - alias the old class name
TrainingManager = HybridTrainingManager